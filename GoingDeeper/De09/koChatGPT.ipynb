{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d76a984",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "#import pandas as pd\n",
    "#import numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a135b9",
   "metadata": {},
   "source": [
    "### 1. Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c8a3314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31b74ff4cfd24515ada900cc05a68962",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/1.00k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7db74f408184fc2b6ec0d79dc223626",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/513M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffb28cf47b81482d8876440cdf664e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/2.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# 사전 학습된 모델 불러오기\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "\n",
    "model_org = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer_org = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7c8d090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 공통 입력 문장\n",
    "input_txt = \"바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37bee5a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gpt2': 1024, 'gpt2-medium': 1024, 'gpt2-large': 1024, 'gpt2-xl': 1024, 'distilgpt2': 1024}\n",
      "['▁바람', '도', '▁없는', '▁공중에', '▁수직', '의', '▁파', '문을', '▁내', '이며', '▁고', '요', '히', '▁떨어지는', '▁오동', '잎은', '▁누', '구의', '▁발자', '취', '▁입', '니까', '.']\n"
     ]
    }
   ],
   "source": [
    "# 토큰화 테스트\n",
    "print(tokenizer_org.max_model_input_sizes)\n",
    "\n",
    "tokens_org = tokenizer_org(input_txt).tokens()\n",
    "print(tokens_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd3f29c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇다면 그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리요?\"\n",
      "\"그건 무슨 소리\n"
     ]
    }
   ],
   "source": [
    "# 모델 생성 결과 테스트\n",
    "max_length=128\n",
    "\n",
    "input_ids_org = tokenizer_org(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "output_org = model_org.generate(input_ids_org, max_length=max_length, do_sample=False)\n",
    "print(tokenizer_org.decode(output_org[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8a3601b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그렇지 않습니다.\"\n",
      "\"어떻게 된 일입니까?\"\n",
      "그녀는 고개를 갸웃거렸다.\n",
      "\"아니, 그게 무슨 말씀이신지 모르겠습니다만.\"\n",
      "\"무슨 말씀인지 알 수가 없군요.\"\n",
      "아무런 대답도 하지 않은 채 그녀는 고개를 끄덕였다.\n",
      "\"그래, 알았어.\"\n",
      "그녀의 눈에서 눈물이 주르륵 흘러내렸다.\n",
      "그녀가 다시 입을 열었다.\n",
      "\"정말 죄송합니다, 고마워요, 고맙습니다\"\n",
      "\"\n",
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.\"\n",
      "\"저것이 누구일까. 그것은 누구인가?\" \"모르겠지만.\"\n",
      "\"누가 저것을 보고 있는지 모르겠습니다.\"\n",
      "\"선생님도 알고 있겠지요. 그러나 저는 선생님이 아니죠. 선생은 누구인지조차 모르는 모양입니다\"라고 말을 이었다.\n",
      "\"그렇다고 저것이 선생님을 뜻하는 것인 줄 알았어요, 그렇지 않습니까. 선생님께서는 저기 계실 때 저의 존재를 알고 계시는 겁니다. 아니요, 선생님은 어디에 계신지 아세요\" 하고\n",
      "바람도 없는 공중에 수직의 파문을 내이며 고요히 떨어지는 오동잎은 누구의 발자취 입니까.'\n",
      "\"그런데 이게 웬일입니까?\"\n",
      "\"아니야. 그게 무슨 소리냐고.\"\n",
      "\"그래, 그건 너희들끼리 얘기하는 거 아니냐. 난 그 얘기를 들은 적이 없단다.\"\n",
      "그렇게 말하고는,\n",
      "\"이런, 그런 얘기는 들어본 적이 없다.\"\n",
      "그러자 그제야 저도 모르게 고개를 끄덕였다.\n",
      "\"무슨 소리야, 그거야.\"\n",
      "그때부터 저는 한숨을 내쉬기 시작했다.\n",
      "\"정말\n"
     ]
    }
   ],
   "source": [
    "# generate 함수 옵션별 테스트 결과 비교\n",
    "output_beam1 = model_org.generate(input_ids_org, max_length=max_length, num_beams=10, no_repeat_ngram_size=2, do_sample=False)\n",
    "print(tokenizer_org.decode(output_beam1[0]))\n",
    "\n",
    "output_beam2 = model_org.generate(input_ids_org, max_length=max_length, num_beams=7, no_repeat_ngram_size=2, do_sample=True, temperature=2.0, top_k=50)\n",
    "print(tokenizer_org.decode(output_beam2[0]))\n",
    "\n",
    "output_beam3 = model_org.generate(input_ids_org, max_length=max_length, num_beams=7, no_repeat_ngram_size=2, do_sample=True, top_p=0.90)\n",
    "print(tokenizer_org.decode(output_beam3[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f69d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장\n",
    "model_org.save_pretrained('./model/output_1_org')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388e201d",
   "metadata": {},
   "source": [
    "### 2. SFT(Supervised Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5f5c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import logging\n",
    "import json\n",
    "import copy\n",
    "from torch.utils.data import Dataset\n",
    "from typing import Optional, Dict, Sequence\n",
    "from dataclasses import dataclass\n",
    "#from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34daed84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFT를 위한 기본 모델 불러오기\n",
    "model_sft = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer_sft = AutoTokenizer.from_pretrained(model_name,\n",
    "        bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>', padding_side=\"right\", model_max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "264bbe5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SFT_dataset(Dataset):\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38a59fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "\n",
    "        return dict(input_ids=input_ids, labels=labels, attention_mask=input_ids.ne(self.tokenizer.pad_token_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0d7ae778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋 준비\n",
    "train_dataset_sft = SFT_dataset(data_path_1_SFT='/aiffel/KoChatGPT/data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer_sft)\n",
    "data_collator_sft = DataCollatorForSupervisedDataset(tokenizer=tokenizer_sft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "964bb10b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 06:21, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.984100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.687200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 학슴을 위한 파마리터 설정, 학습, 결과 저장\n",
    "training_args_sft = TrainingArguments(\n",
    "    output_dir=\"./test\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "\n",
    "trainer_sft = Trainer(model=model_sft, args=training_args_sft, data_collator=data_collator_sft, train_dataset=train_dataset_sft)\n",
    "\n",
    "trainer_sft.train()\n",
    "model_sft.save_pretrained('./model/output_1_sft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ca91a22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************\n",
      "질문(prompt): 불고기용 고기 한우에요?\n",
      "\n",
      "대답(기본 모델): 불고기용 고기 한우에요?!\n",
      "저거 쫄깃하면서도 매콤하며 달달한 육향이 살아있고\n",
      "다이어트하는 동안에도 넘 잘어울리는 식감이였어요!\n",
      "#다히먹방<unk>#먹스타그램 #존맛\n",
      "#존맛탱 #불고기 #먹부림 #맛있다\n",
      "#먹방 #먹짤 #f4follow #foodporn #l4l #선팔 #맞팔해요\n",
      "#소통 #팔로우 #일상 #소통스타그램\n",
      "\n",
      "대답(SFT  모델): 불고기용 고기 한우에요?\\n\\n저는 인공지능 어시스턴트이기 때문에 고기를 먹을 수 없습니다. 하지만, 인터넷에서 검색해보시는 것을 추천드립니다!\\nSure, I am an AI language model, I cannot provide more context\n",
      "\n",
      "********************\n",
      "질문(prompt): 리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "대답(기본 모델): 리처드 닉슨이 43대 부통령직을 수행한 년도는?.\n",
      "1999년 5월 22일 ~ 2004년 6월 16일.\n",
      "브라이언 맥클레인 ( 1997-1997 ) : 미 공화당 대선 후보.\n",
      "토니 블레어 ( 1993-2003 ) : 미국 연방 하원선거\n",
      "마이크 허커 ( 1998-2004 ) : 뉴욕 주지사 ( 공화당 )\n",
      "제프리 스톤 ( 2000-2003 )\n",
      "찰리 폴슨 ( 2002-2011년 ) : 대만 인민위원회 위원장\n",
      "제이슨 리 ( 2005-2010년 ) : 미국 뉴욕 주지사 ( 민주당 )\n",
      "조셉 하딩턴 ( 2006\n",
      "\n",
      "대답(SFT  모델): 리처드 닉슨이 43대 부통령직을 수행한 년도는?\\n\\n닉슨은 40대 부통령직을 수행하지 않았습니다. 그러나 그는 1960년대 후반부터 1970년대 초반까지 부통령직을 수행했습니다.子供養)使用: 英祖僧士, 英祖宗軍師, 永祖宗軍士, 榮祖宗軍事, 永\n",
      "\n",
      "********************\n",
      "질문(prompt): 시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "대답(기본 모델): 시카고 오헤어 국제공항은 어디에 있어?\" 등의 반응을 보였다.</d> 중국 상하이의 부동산 재벌인 리옌훙(李彦宏)이 자신의 부인 펑후청(馮惠靑)씨의 부동산 중개업소를 직접 방문,부동산 매매를 중개해 주는 등 적극 나선 데 이어 부인 펑후청씨와 함께 베이징의 주택 임대사업을 하는 등 부동산 관련 업무를 담당하고 있는 왕칭(王慶) 부부장이 직접 부동산 거래에 뛰어들었다.\n",
      "리부부장의 부인은 지난 8일 상하이의 부동산 임대업을 담당해 온 리샤오보(黎曉波)를 방문해 펑부부장의 부동산을 소개해 주었으며 리부장은 펑부부장이 부동산 중개업을 하는 것은 물론\n",
      "\n",
      "대답(SFT  모델): 시카고 오헤어 국제공항은 어디에 있어?\\n\\n저는 인공지능 어시스턴트이기 때문에 정확한 답변을 드리기 어렵습니다. 하지만 \"오헤어 국제공항\"은 대한민국 서울특별시 중구 을지로에 위치해 있습니다. 따라서 해당 정보를 얻기 위해서는 해당 공항의 공식 웹사이트를 방문하시는 것이 좋을 것 같습니다. 감사합니다.述)述\n",
      "\n",
      "********************\n",
      "질문(prompt): 오늘 미세먼지 어때?\n",
      "\n",
      "대답(기본 모델): 오늘 미세먼지 어때? 뭐 뭐 일단은 그건 사실이지. <unk> 그런 거 있잖아. 그래서 어~ 미세먼지가 많이 나오는 게 사실이라면. 그거보다 훨씬 더 심각한 것 아니야? 어~ 그~ 그런 건 아니지만은. 그런 게 있겠지. 네. 네. 근데 이제 저희 저희가 저희 방송에서도 좀 이슈가 되지 않았나? 봤는데 그거보다는 좀 저희가 약간 더 이슈가 되겠네요. 그거보다는 더 심각하지. 네. 그런데. 어~ 이~ 미세먼지 같은 경우에는 이~ 저기 그~ 뭐 그~\n",
      "\n",
      "대답(SFT  모델): 오늘 미세먼지 어때?\\n\\n저는 인공지능 챗봇이기 때문에 미세먼지 관련 정보를 알 수 없습니다. 그러나 미세먼지 농도가 높은 날에는 실외활동을 자제하는 것이 좋습니다. 또한 미세먼지가 심한 날에는 공기청정기를 사용하는 것이 좋습니다. 또한, 미세먼지 마스크를 착용해 미세먼지를 줄이는\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 결과 비교\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "# 기본 모델\n",
    "list_result_org = []\n",
    "for prompt in list_prompt:\n",
    "    input_ids = tokenizer_org(prompt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
    "    output = model_org.generate(input_ids, max_length=max_length, num_beams=4, no_repeat_ngram_size=4, do_sample=True, temperature=2.0, top_k=50)\n",
    "    list_result_org.append(tokenizer_org.decode(output[0]))\n",
    "\n",
    "# SFT 모델\n",
    "generator_sft = pipeline('text-generation', model='./model/output_1_sft', tokenizer=tokenizer_sft)\n",
    "\n",
    "generation_args = dict(\n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n\n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# PROMPT_DICT = {\n",
    "#     \"prompt_input\": (\n",
    "#         \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# prompt_dict = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result_sft = generator_sft(list_prompt, **generation_args)\n",
    "\n",
    "# 결과 비교\n",
    "for prompt, result_org, result_sft in zip(list_prompt, list_result_org, list_result_sft):\n",
    "    print('*' * 20)\n",
    "    print(f\"질문(prompt): {prompt}\")\n",
    "    print()\n",
    "    print(f\"대답(기본 모델): {result_org}\")\n",
    "    print()\n",
    "    print(f\"대답(SFT  모델): {result_sft[0]['generated_text']}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b4207b2",
   "metadata": {},
   "source": [
    "#### 분석\n",
    "- 사전 학습만 이뤄진 모델과 fine-tuning된 모델의 비교에서 상대적으로 SFT 모델이 조금 더 낫다는 생각이지만, 만족스러운 응답을 보이는 것은 아님\n",
    "- generation 관련해서 공통적으로 변경 가능한 파라미터가 있는데(num_beams, no_repeat_ngram_size, top_k 등), 해당 파라미터의 변경에 따른 결과 비교도 필요해 보임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e9e332",
   "metadata": {},
   "source": [
    "## 회고\n",
    "- 상업적으로도 널리 이용되고 있는 chatGPT의 동작 원리를 이해할 수 있는 시간이었음.\n",
    "- Reward Model을 학습하는 단계에서 라이브러리를 불러올 때 에러가 발생하였는데, 해당 이슈를 해결하지 못해, 이후 단계를 직접 테스트해보지 못한 것이 아쉬움\n",
    "- Node에서의 학습을 통해 강화학습 단계에서 정성적인 성능이 크게 향상되는 것을 확인하였음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5050b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
